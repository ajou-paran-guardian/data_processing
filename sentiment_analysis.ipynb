{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPIOeLOKO9qGSx+WPHui7wS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**실행 후, 추출된 파일 내용 의미**\n","1. text: HTML코드에서 추출한 문구\n","2. sentiment_label: 아래 단계 중 하나\n","3. sentiment_score: 감정의 강도 또는 확신도 (값이 높을수록 해당 감정 레이블에 대한 확신이 크다는 뜻)\n","\n","<단계>\n","\n","*   1 star: 매우 부정적인 감정\n","*   2 stars: 다소 부정적인 감정\n","*   3 stars: 중립적인 감정\n","*   4 stars: 긍정적인 감정\n","*   5 stars: 매우 긍정적인 감정\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"h64NozOcPdGJ"}},{"cell_type":"code","source":["# 1. 구글 드라이브 연결\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"N8vMcXx3KJRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 감정 분석 파이프라인 로드 (구글 드라이브 연결할 때마다 실행)\n","import os\n","import json\n","import re\n","from transformers import pipeline\n","sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"],"metadata":{"id":"QPRX55R7WvNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파일 경로 설정(각자 알아서)\n","input_file_path = '/content/drive/MyDrive/test_result(1~250).json'  # 입력 파일 경로\n","output_file_path = '/content/drive/MyDrive/sentiment_results(1~250).json'  # 출력 파일 경로"],"metadata":{"id":"6KyFtZbZLP_L","executionInfo":{"status":"ok","timestamp":1730903375947,"user_tz":-540,"elapsed":416,"user":{"displayName":"신은별","userId":"14839074258464084909"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# HTML에서 텍스트 추출 및 감정 분석 함수 정의\n","\n","def extract_text_from_html(html_code):\n","    \"\"\"HTML 코드에서 텍스트를 추출하고 불필요한 공백 제거\"\"\"\n","    # 스크립트, 스타일, 주석 제거\n","    html_code = re.sub(r'<script.*?>.*?</script>', '', html_code, flags=re.DOTALL)\n","    html_code = re.sub(r'<style.*?>.*?</style>', '', html_code, flags=re.DOTALL)\n","    html_code = re.sub(r'<!--.*?-->', '', html_code, flags=re.DOTALL)\n","\n","    # HTML 태그 제거 및 공백 정리\n","    text = re.sub('<[^<]+?>', '', html_code)  # HTML 태그 제거\n","    text = re.sub(r'\\s+', ' ', text)  # 여러 공백을 하나의 공백으로 치환\n","    return text.strip()\n","\n","\n","def split_text(text, max_length=512):\n","    \"\"\"텍스트를 max_length 이하의 조각으로 문장이 끊기지 않도록 나누는 함수\"\"\"\n","    # 텍스트를 단어 단위로 분할\n","    words = text.split()\n","    chunks = []\n","    chunk = \"\"\n","\n","    for word in words:\n","        # 현재 조각에 단어를 추가했을 때 max_length를 초과하지 않으면 추가\n","        if len(chunk) + len(word) + 1 <= max_length:  # +1은 공백 추가를 고려한 것\n","            chunk += (\" \" + word if chunk else word)\n","        else:\n","            # 현재 조각이 max_length에 가까우면 조각을 추가하고 새로운 조각 시작\n","            chunks.append(chunk)\n","            chunk = word\n","\n","    # 마지막 조각 추가\n","    if chunk:\n","        chunks.append(chunk)\n","\n","    return chunks\n","\n","def analyze_sentiment(data):\n","    \"\"\"JSON 데이터에서 HTML 텍스트를 추출하고 감정 분석을 수행\"\"\"\n","    results = {}\n","    for key, content in data.items():\n","        html_code = content.get(\"HTML\", \"\")\n","        text = extract_text_from_html(html_code)\n","\n","        # 감정 분석 수행\n","        if text:\n","            text_chunks = split_text(text, max_length=256)\n","            sentiments = [sentiment_analyzer(chunk)[0] for chunk in text_chunks]\n","\n","            # 각 조각의 감정 분석 결과를 결합 (예: 'label'을 기준으로 가장 많이 나타난 레이블 선택)\n","            labels = [sent[\"label\"] for sent in sentiments]\n","            combined_label = max(set(labels), key=labels.count)\n","\n","            # 점수는 평균화하여 결합\n","            scores = [sent[\"score\"] for sent in sentiments if sent[\"label\"] == combined_label]\n","            average_score = sum(scores) / len(scores)\n","\n","            results[key] = {\n","                \"text\": text,\n","                \"sentiment_label\": combined_label,\n","                \"sentiment_score\": average_score\n","            }\n","    return results\n","\n","# JSON 파일 로드\n","with open(input_file_path, 'r', encoding='utf-8') as file:\n","    data = json.load(file)\n","\n","# 감정 분석 수행 및 결과 저장\n","sentiment_results = analyze_sentiment(data)\n","with open(output_file_path, 'w', encoding='utf-8') as output_file:\n","    json.dump(sentiment_results, output_file, ensure_ascii=False, indent=4)\n","\n","print(f\"Sentiment analysis results saved to {output_file_path}\")"],"metadata":{"id":"OCUHXOJ4AtEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","\n","# JSON 파일 로드\n","with open(output_file_path, 'r', encoding='utf-8') as file:\n","    json_data = json.load(file)\n","\n","# 판다스 데이터프레임으로 변환\n","df = pd.DataFrame(json_data)\n","\n","# 출력 설정 변경\n","pd.set_option('display.max_columns', None)  # 모든 열 표시\n","pd.set_option('display.width', None)        # 자동으로 너비 조정\n","\n","# 결과 출력\n","df.T"],"metadata":{"id":"VB9fuQwPQ1bZ"},"execution_count":null,"outputs":[]}]}