{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["1b0CyhxB8J4p","hYmhLnEX01Mm","Amps2oCBwj3Q"],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNHmA64d4ql2xRAa5f9lHc0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ab2LIYzBImBp","executionInfo":{"status":"ok","timestamp":1732111808963,"user_tz":-540,"elapsed":61175,"user":{"displayName":"신은별","userId":"14839074258464084909"}},"outputId":"1b5ad09c-ae7c-4439-8d55-b2f62b929a1b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["전처리 코드 자동 실행"],"metadata":{"id":"mZyC-NjzImjW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aqKII61IcIU"},"outputs":[],"source":["import nbformat\n","from google.colab import files\n","from google.colab.output import eval_js\n","from IPython import get_ipython\n","from IPython.core.interactiveshell import InteractiveShell\n","\n","# .ipynb 파일 경로\n","notebook_path = '/content/drive/MyDrive/전처리ver6.0.ipynb'\n","\n","# .ipynb 파일 읽기\n","with open(notebook_path) as f:\n","    notebook = nbformat.read(f, as_version=4)\n","\n","# 셀 단위로 실행\n","shell = InteractiveShell.instance()\n","for cell in notebook.cells:\n","    if cell.cell_type == 'code':\n","        shell.run_cell(cell.source)\n"]},{"cell_type":"markdown","metadata":{"id":"1b0CyhxB8J4p"},"source":["# BERT 기계학습 (다중 분류, static 코드, test 데이터 분리, 토큰화 데이터 저장)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-uq7iiEGSYr"},"outputs":[],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OxZd-9CIcTh"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","from transformers import BertTokenizer\n","from datasets import Dataset\n","from sklearn.metrics import classification_report, accuracy_score, f1_score\n","from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n","import numpy as np\n","\n","# 데이터 전처리 함수\n","def preprocess_data(df):\n","    # HTML과 CSS를 하나의 문자열로 합침\n","    df['HTML_CSS'] = df['HTML+CSS'].apply(lambda x: x if isinstance(x, str) else \"\")\n","\n","    # 라벨 인코딩\n","    label_encoder = LabelEncoder()\n","\n","    # '다크패턴 여부' 대신 'type' 컬럼을 다중 클래스 레이블로 변환\n","    df['label'] = label_encoder.fit_transform(df['type'])  # 'type' 컬럼을 라벨로 사용\n","\n","    # '추출한 텍스트'와 'HTML+CSS'도 사용하도록\n","    df['HTML_text'] = df['추출한 텍스트']  # 텍스트를 'HTML_CSS'와 함께 사용할 수 있도록\n","\n","    df['HTML_match'] = df['Static HTML 일치도']\n","\n","    return df[['HTML_CSS', 'HTML_text', 'HTML_match', 'label']]\n","\n","# BERT의 기본 토크나이저 로드\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1003,"status":"ok","timestamp":1732111978322,"user":{"displayName":"신은별","userId":"14839074258464084909"},"user_tz":-540},"id":"IHRqdQEPIe6I"},"outputs":[],"source":["# 토큰화 함수 정의 (HTML_CSS만 사용)\n","def tokenize_html_css(examples):\n","    return tokenizer(examples['HTML_CSS'], padding='max_length', truncation=True, max_length=255)\n","\n","# 토큰화 함수 정의 (HTML_text만 사용)\n","def tokenize_html_text(examples):\n","    return tokenizer(examples['HTML_text'], padding='max_length', truncation=True, max_length=255)\n","\n","# 학습 데이터 전처리\n","df_train_processed = preprocess_data(df_train)\n","train_data = Dataset.from_pandas(df_train_processed[['HTML_CSS', 'HTML_text', 'HTML_match', 'label']])\n","\n","# 테스트 데이터 전처리\n","df_test_processed = preprocess_data(df_test)\n","test_data = Dataset.from_pandas(df_test_processed[['HTML_CSS', 'HTML_text', 'HTML_match', 'label']])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIEBt2gX9tAX"},"outputs":[],"source":["# 학습 데이터에서 'label' 컬럼의 분포 확인\n","train_df = train_data.to_pandas()  # Dataset을 pandas DataFrame으로 변환\n","train_label_counts = train_df['label'].value_counts()\n","print(\"Train data label distribution:\")\n","print(train_label_counts)\n","\n","# 테스트 데이터에서 'label' 컬럼의 분포 확인\n","test_df = test_data.to_pandas()  # Dataset을 pandas DataFrame으로 변환\n","test_label_counts = test_df['label'].value_counts()\n","print(\"Test data label distribution:\")\n","print(test_label_counts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3-JTUDkI5QR"},"outputs":[],"source":["# 토큰화 함수 적용\n","train_data_html_css = train_data.map(tokenize_html_css, batched=True, num_proc=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EY9pmwKCI6ob"},"outputs":[],"source":["test_data_html_css = test_data.map(tokenize_html_css, batched=True, num_proc=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a43-2l5kI8rV"},"outputs":[],"source":["train_data_html_text = train_data.map(tokenize_html_text, batched=True, num_proc=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEvlWpaCI-4f"},"outputs":[],"source":["test_data_html_text = test_data.map(tokenize_html_text, batched=True, num_proc=4)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"YSS4RKXa8QjH","executionInfo":{"status":"ok","timestamp":1732114155825,"user_tz":-540,"elapsed":449,"user":{"displayName":"신은별","userId":"14839074258464084909"}}},"outputs":[],"source":["# [SEP] 토큰의 ID를 가져오기\n","sep_token_id = tokenizer.sep_token_id\n","\n","# 데이터셋 결합 함수 정의\n","def combine_datasets(dataset1, dataset2):\n","    # 'input_ids'와 'attention_mask'를 합치기\n","    combined_input_ids = [d1 + [sep_token_id] + d2 for d1, d2 in zip(dataset1['input_ids'], dataset2['input_ids'])]\n","    combined_attention_mask = [m1 + [sep_token_id] + m2 for m1, m2 in zip(dataset1['attention_mask'], dataset2['attention_mask'])]\n","\n","    combined_html_match = dataset1['HTML_match']\n","\n","    # 레이블과 'type' 컬럼은 동일하게 유지\n","    combined_dataset = {\n","        'input_ids': combined_input_ids,\n","        'attention_mask': combined_attention_mask,\n","        'HTML_match': combined_html_match,\n","        'label': dataset1['label'],  # 다중 분류 레이블 그대로 사용\n","    }\n","\n","    return Dataset.from_dict(combined_dataset)\n","\n","# 결합된 학습 및 테스트 데이터 생성\n","combined_train_data = combine_datasets(train_data_html_css, train_data_html_text)\n","combined_test_data = combine_datasets(test_data_html_css, test_data_html_text)\n"]},{"cell_type":"markdown","source":["# 아래 두 코드는 추가 데이터가 들어올 것 같은때 사용\n","# 진짜 주의!! 직접 파일명 변경해서 실행할 것!! 아니면 덮여쓰기됨..\n","토큰화한 데이터를 폴더에 저장하고 새로운 데이터가 들어왔을때 토큰화한 후에 폴더에 저장함, 학습시킬 때는 그 폴더 안에 존재하는 데이터들 합쳐서 사용함\n"],"metadata":{"id":"bq4vtiFwejUO"}},{"cell_type":"code","source":["import os\n","import json\n","\n","# JSON 파일 저장 함수 정의\n","def save_dataset_to_json(dataset, file_path):\n","\n","    # 폴더가 없으면 생성\n","    folder_path = os.path.dirname(file_path)\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","    # Dataset 객체를 딕셔너리로 변환\n","    dataset_dict = dataset.to_dict()\n","\n","    # JSON 파일로 저장\n","    with open(file_path, 'w', encoding='utf-8') as f:\n","        json.dump(dataset_dict, f, ensure_ascii=False, indent=4)\n","\n","    print(f\"Dataset saved to {file_path}\")\n","\n","\n","'''\n","아래코드의 파일명 변경하기! 각각\n","\"./drive/MyDrive/토큰화된 학습 데이터/{여기 변경}.json\"\n","\"./drive/MyDrive/토큰화된 테스트 데이터/{여기 변경}.json\"\n","'''\n","\n","# 학습 데이터 저장\n","save_dataset_to_json(combined_train_data, \"./drive/MyDrive/토큰화된 학습 데이터/train_data1.json\")\n","\n","# 테스트 데이터 저장\n","save_dataset_to_json(combined_test_data, \"./drive/MyDrive/토큰화된 테스트 데이터/test_data1.json\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FzTCmZOC_y_","executionInfo":{"status":"ok","timestamp":1732114269434,"user_tz":-540,"elapsed":474,"user":{"displayName":"신은별","userId":"14839074258464084909"}},"outputId":"ac8e9035-0996-4f73-8fa2-8c8b28860356"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset saved to ./drive/MyDrive/토큰화된 학습 데이터/train_data1.json\n","Dataset saved to ./drive/MyDrive/토큰화된 테스트 데이터/test_data1.json\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","from datasets import Dataset\n","\n","# JSON 파일을 로드하여 하나의 Dataset으로 결합하는 함수\n","def load_and_combine_json_files(folder_path):\n","    # 폴더 내 모든 JSON 파일 리스트 가져오기\n","    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n","\n","    # JSON 파일들을 Dataset 객체로 변환하고 결합\n","    combined_dataset = None\n","    for json_file in json_files:\n","        file_path = os.path.join(folder_path, json_file)\n","\n","        # JSON 파일 로드\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            dataset_dict = json.load(f)\n","\n","        # Dataset 객체로 변환\n","        dataset = Dataset.from_dict(dataset_dict)\n","\n","        # 첫 번째 파일은 그대로 두고, 이후 파일은 합침\n","        if combined_dataset is None:\n","            combined_dataset = dataset\n","        else:\n","            combined_dataset = concatenate_datasets([combined_dataset, dataset])\n","\n","    return combined_dataset\n","\n","# 학습 데이터와 테스트 데이터 로드 및 결합\n","combined_train_data = load_and_combine_json_files(\"./drive/MyDrive/토큰화된 학습 데이터\")\n","combined_test_data = load_and_combine_json_files(\"./drive/MyDrive/토큰화된 테스트 데이터\")"],"metadata":{"id":"k-OKKEG_EJbC","executionInfo":{"status":"ok","timestamp":1732114427271,"user_tz":-540,"elapsed":469,"user":{"displayName":"신은별","userId":"14839074258464084909"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# 확인용: 합쳐진 데이터셋 정보 출력\n","print(f\"Combined train data: {combined_train_data}\")\n","print(f\"Combined test data: {combined_test_data}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WFZ4E9CYELvU","executionInfo":{"status":"ok","timestamp":1732114437369,"user_tz":-540,"elapsed":1008,"user":{"displayName":"신은별","userId":"14839074258464084909"}},"outputId":"3a2d9fbd-39a3-4510-975f-63dc2325ce16"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined train data: Dataset({\n","    features: ['input_ids', 'attention_mask', 'HTML_match', 'label'],\n","    num_rows: 196\n","})\n","Combined test data: Dataset({\n","    features: ['input_ids', 'attention_mask', 'HTML_match', 'label'],\n","    num_rows: 120\n","})\n"]}]},{"cell_type":"markdown","source":["# BERT 모델 학습시키고 성능 평가, 시각화 코드"],"metadata":{"id":"2iDCtO6CGDLN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UncpLF-8UU_"},"outputs":[],"source":["# type_encoded에 대한 라벨을 사람이 이해할 수 있는 이름으로 매핑\n","# type_labels = {\n","#     0: \"다크패턴 아닌 데이터\",\n","#     1: \"방해형\",\n","#     2: \"압박형\",\n","#     3: \"오도형\"\n","# }\n","\n","type_labels = {\n","    0: \"다크패턴 아닌 데이터\",\n","    1: \"압박형\",\n","    2: \"오도형\",\n","    3: \"방해형\"\n","}\n","\n","# 다중 클래스 F1, 정확도 및 유형별 정확도를 계산하는 함수 정의\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)  # 예측 레이블 계산\n","\n","    # 전체 정확도 및 F1 점수 계산\n","    acc = accuracy_score(labels, predictions)\n","    f1 = f1_score(labels, predictions, average='macro')\n","\n","    # Type별 정확도 계산\n","    type_accuracies = {}\n","    for i in range(4):  # 0~3까지 각 클래스에 대해\n","        type_mask = (labels == i)\n","        if np.any(type_mask):  # 해당 타입에 데이터가 있는 경우에만 계산\n","            type_acc = accuracy_score(labels[type_mask], predictions[type_mask])\n","            type_accuracies[f\"{type_labels[i]}_accuracy\"] = type_acc\n","\n","    return {\n","        \"accuracy\": acc,\n","        \"f1\": f1,\n","        **type_accuracies  # Type별 정확도 추가\n","    }\n","\n","# 모델 초기화 (4개의 클래스로 설정)\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n","\n","# 훈련 설정\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=3e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    lr_scheduler_type=\"linear\",\n","    report_to='none'\n",")\n","\n","# Trainer 초기화\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=combined_train_data,\n","    eval_dataset=combined_test_data,\n","    compute_metrics=compute_metrics\n",")\n","\n","# 학습 시작\n","trainer.train()\n","\n","# 모델 평가 및 결과 예측\n","eval_results = trainer.predict(combined_test_data)\n","\n","# 예측값과 실제값\n","predictions = np.argmax(eval_results.predictions, axis=-1)\n","labels = eval_results.label_ids\n","\n","# classification report 출력 (표 형식으로 직접 출력)\n","print(\"Classification Report:\")\n","print(classification_report(labels, predictions, target_names=[type_labels[i] for i in range(4)]))\n","\n","# compute_metrics 함수 직접 호출\n","metrics = compute_metrics((eval_results.predictions, eval_results.label_ids))\n","\n","# 전체 정확도와 각 유형에 대한 정확도 출력\n","print(f\"\\n전체 정확도 (Accuracy): {metrics['accuracy']:.2f}\")\n","print(f\"F1 스코어 (F1 Score): {metrics['f1']:.2f}\")\n","for label, accuracy in metrics.items():\n","    if \"_accuracy\" in label:\n","        print(f\"{label}: {accuracy:.2f}\")\n"]},{"cell_type":"markdown","source":["아래 두 코드는 시각화 자료"],"metadata":{"id":"tytss_SHF47c"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","\n","# 다중 클래스 레이블을 이진형으로 변환\n","labels_binarized = label_binarize(labels, classes=[0, 1, 2, 3])\n","predictions_binarized = label_binarize(predictions, classes=[0, 1, 2, 3])\n","\n","# ROC Curve와 AUC 계산 및 시각화\n","plt.figure(figsize=(10, 6))\n","for i in range(4):\n","    fpr, tpr, _ = roc_curve(labels_binarized[:, i], predictions_binarized[:, i])\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=f'Class {type_labels[i]} (AUC = {roc_auc:.2f})')\n","\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # 대각선 (무작위 예측)\n","plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"fe5Af2_bO_aV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","# 혼동 행렬 계산\n","cm = confusion_matrix(labels, predictions)\n","\n","# 혼동 행렬을 히트맵으로 시각화\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[type_labels[i] for i in range(4)], yticklabels=[type_labels[i] for i in range(4)])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.show()\n"],"metadata":{"id":"ZBe1A_WFPEwR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#여기까지가 BERT 기계학습 (다중 분류, static 코드, test 데이터 분리, 토큰화 데이터 저장)"],"metadata":{"id":"lzPJRflkG5jv"}},{"cell_type":"markdown","metadata":{"id":"hYmhLnEX01Mm"},"source":["# BERT 이진 분류 (static 코드는 없는 버전)"]},{"cell_type":"markdown","metadata":{"id":"-sxq1ZlI1RHs"},"source":["**데이터 전처리 및 토큰화 함수 정의**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTwlrghYDZaN"},"outputs":[],"source":["def preprocess_data(df):\n","    # HTML+CSS를 텍스트화\n","    df['HTML_CSS'] = df['HTML+CSS'].apply(lambda x: x if isinstance(x, str) else \"\")\n","\n","    # 라벨 인코딩\n","    label_encoder = LabelEncoder()\n","    df['label'] = label_encoder.fit_transform(df['다크패턴 여부'])\n","\n","    # 반대로 변환\n","    df['label'] = df['label'].apply(lambda x: 1 if x == 0 else 0)\n","\n","    # 'type' 컬럼도 라벨 인코딩, 이중 분류여서 사용은 안 함\n","    df['type_encoded'] = label_encoder.fit_transform(df['type'])\n","\n","    # '추출한 텍스트'\n","    df['HTML_text'] = df['추출한 텍스트']\n","    return df[['HTML_CSS', 'HTML_text', 'label', 'type_encoded']]\n","\n","# 전처리된 데이터\n","df_processed = preprocess_data(df_no_duplicates)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud7KlQ7p5ecA"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# BERT의 기본 토크나이저를 로드\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# 토큰화 함수 정의 (HTML_CSS만 사용)\n","def tokenize_html_css(examples):\n","    return tokenizer(examples['HTML_CSS'], padding='max_length', truncation=True, max_length=256)\n","\n","# 토큰화 함수 정의 (HTML_text만 사용)\n","def tokenize_html_text(examples):\n","    return tokenizer(examples['HTML_text'], padding='max_length', truncation=True, max_length=256)\n"]},{"cell_type":"markdown","metadata":{"id":"BMKyaSwsfQES"},"source":["**아래 코드가 토큰화하는 코드**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEQ5n8Xu3gpf"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","# 학습 데이터와 테스트 데이터 나누기\n","train_texts_html_css, test_texts_html_css, train_labels, test_labels, train_type_encodeds, test_type_encodeds, train_texts_html_text, test_texts_html_text = train_test_split(\n","    df_processed['HTML_CSS'].tolist(),\n","    df_processed['label'].tolist(),\n","    df_processed['type_encoded'].tolist(),\n","    df_processed['HTML_text'].tolist(),  # HTML_text도 포함\n","    test_size=0.2, random_state=42, stratify=df_processed['type_encoded']\n",")\n","\n","# 데이터셋 생성\n","train_data = Dataset.from_dict({\n","    'HTML_CSS': train_texts_html_css,\n","    'HTML_text': train_texts_html_text,\n","    'label': train_labels,\n","    'type': train_type_encodeds\n","})\n","\n","test_data = Dataset.from_dict({\n","    'HTML_CSS': test_texts_html_css,\n","    'HTML_text': test_texts_html_text,\n","    'label': test_labels,\n","    'type': test_type_encodeds\n","})\n","\n","# 데이터셋에 각각의 토큰화 적용 (병렬 처리 활성화)\n","train_data_html_css = train_data.map(tokenize_html_css, batched=True, num_proc=4)\n","test_data_html_css = test_data.map(tokenize_html_css, batched=True, num_proc=4)\n","\n","train_data_html_text = train_data.map(tokenize_html_text, batched=True, num_proc=4)\n","test_data_html_text = test_data.map(tokenize_html_text, batched=True, num_proc=4)"]},{"cell_type":"markdown","metadata":{"id":"IpomsGPd1qyF"},"source":["**아래 코드는 토큰화한 데이터 합치기(html+css, 추출한 텍스트)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXdzo-fO1i6C"},"outputs":[],"source":["from datasets import Dataset, concatenate_datasets\n","\n","# 토큰화된 데이터셋 결합 함수 정의\n","def combine_datasets(dataset1, dataset2):\n","    combined_dataset = {\n","        'input_ids': [d1 + d2 for d1, d2 in zip(dataset1['input_ids'], dataset2['input_ids'])],\n","        'attention_mask': [m1 + m2 for m1, m2 in zip(dataset1['attention_mask'], dataset2['attention_mask'])],\n","        'label': dataset1['label'],  # 동일한 레이블 사용\n","        'type': dataset1['type']  # 'type'도 함께 결합\n","    }\n","    return Dataset.from_dict(combined_dataset)\n","\n","# 결합된 학습 및 테스트 데이터 생성\n","combined_train_data = combine_datasets(train_data_html_css, train_data_html_text)\n","combined_test_data = combine_datasets(test_data_html_css, test_data_html_text)"]},{"cell_type":"markdown","source":["**BERT 이중분류 학습하는 코드 + 성능 평가**"],"metadata":{"id":"x27UQPQ3JthG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcQQwxfB1YtM"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc\n","import matplotlib.pyplot as plt\n","from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n","\n","# 평가 지표 계산 함수 정의\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    acc = accuracy_score(labels, predictions))\n","    f1 = f1_score(labels, predictions, average='binary')\n","    roc_auc = roc_auc_score(labels, logits[:, 1])\n","\n","    return {\"accuracy\": acc, \"f1\": f1, \"roc_auc\": roc_auc}\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","# 훈련 설정, 하이퍼파라미터 튜닝 옵션\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    lr_scheduler_type=\"linear\",\n","    report_to='none'\n",")\n","\n","\n","# Trainer 초기화\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=combined_train_data,\n","    eval_dataset=combined_test_data,\n","    compute_metrics=compute_metrics  # 평가 지표 계산 함수 추가\n",")\n","\n","# 학습 시작\n","trainer.train()\n","\n","# 모델 평가 후 시각화\n","# 평가 결과 얻기\n","eval_results = trainer.predict(combined_test_data)\n","\n","# 실제 레이블(labels)과 예측 확률(probs) 추출\n","probs = eval_results.predictions[:, 1]\n","labels = eval_results.label_ids\n","\n","# ROC 곡선과 AUC 계산\n","fpr, tpr, thresholds = roc_curve(labels, probs)\n","roc_auc = auc(fpr, tpr)\n","\n","# ROC 곡선 시각화\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n","plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel(\"False Positive Rate (FPR)\")\n","plt.ylabel(\"True Positive Rate (TPR)\")\n","plt.title(\"ROC Curve\")\n","plt.legend(loc=\"lower right\")\n","plt.grid()\n","plt.show()\n","\n","# 정확도, F1 점수, AUC 출력\n","print(f\"정확도 (Accuracy): {eval_results.metrics['test_accuracy']:.2f}\")\n","print(f\"F1 점수 (F1 Score): {eval_results.metrics['test_f1']:.2f}\")\n","print(f\"ROC-AUC (AUC Score): {eval_results.metrics['test_roc_auc']:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Amps2oCBwj3Q"},"source":["# CodeBert (다중분류 + static 코드 없음)"]},{"cell_type":"markdown","source":["텍스트와 코드를 분할하는 함수 정의\n","분할하는 이유는 CodeBERT 모델에서 입력 토큰의 최대 길이가 512개"],"metadata":{"id":"lP8t6s-FLaUF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiDB0Y2hvg3A"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import RobertaTokenizer  # CodeBERT의 RoBERTa 기반 토크나이저\n","\n","# CodeBERT 토크나이저 로드\n","tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n","\n","def split_text(text, max_length=256):\n","    \"\"\"\n","    텍스트를 주어진 길이(max_length)로 분할하는 함수.\n","    \"\"\"\n","    words = text.split()\n","    chunks = []\n","    chunk = \"\"\n","\n","    for word in words:\n","        if len(chunk) + len(word) + 1 <= max_length:\n","            chunk += (\" \" + word if chunk else word)\n","        else:\n","            chunks.append(chunk)\n","            chunk = word\n","    if chunk:\n","        chunks.append(chunk)\n","    return chunks\n","\n","\n","def split_code(code, max_length=256):\n","    \"\"\"\n","    코드도 텍스트와 비슷하게 분할하는 함수.\n","    \"\"\"\n","    lines = code.split(\"\\n\")\n","    chunks = []\n","    chunk = \"\"\n","\n","    for line in lines:\n","        if len(chunk) + len(line) + 1 <= max_length:\n","            chunk += (\" \" + line if chunk else line)\n","        else:\n","            chunks.append(chunk)\n","            chunk = line\n","    if chunk:\n","        chunks.append(chunk)\n","    return chunks\n"]},{"cell_type":"markdown","source":["데이터 분할하는 함수"],"metadata":{"id":"_2LkAthjMNv8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8Iexk6dMbOL"},"outputs":[],"source":["def split_data(df, max_chunk_length=256):\n","    \"\"\"\n","    DataFrame의 텍스트와 코드를 분할하는 함수.\n","    \"\"\"\n","    split_text_data = []\n","    split_code_data = []\n","    labels = []\n","\n","    # 라벨 인코더 생성\n","    label_encoder = LabelEncoder()\n","    df['type_encoded'] = label_encoder.fit_transform(df['type'])  # 'type' 컬럼을 숫자로 인코딩\n","\n","    for idx, row in df.iterrows():\n","        text = row['추출한 텍스트']\n","        code = row['HTML+CSS']\n","        label = row['type_encoded']  # 라벨: type 컬럼\n","\n","        # 텍스트와 코드 분할\n","        text_chunks = split_text(text, max_chunk_length)\n","        code_chunks = split_code(code, max_chunk_length)\n","\n","        split_text_data.append(text_chunks)\n","        split_code_data.append(code_chunks)\n","        labels.append(label)\n","\n","    return split_text_data, split_code_data, labels\n","\n","split_text_data, split_code_data, labels = split_data(df_no_duplicates)"]},{"cell_type":"markdown","source":["너무 데이터가 커서 코랩의 세션이 종료되는 데이터 제외"],"metadata":{"id":"bJuEyXGmL_5d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfPwnYIqiNkk"},"outputs":[],"source":["# 20,000,000 이상인 데이터를 제외하고 따로 모으는 코드\n","excluded_data_indices = []\n","excluded_data = []  # 제외된 데이터를 저장할 리스트\n","remaining_split_text_data = []\n","remaining_split_code_data = []\n","remaining_labels = []\n","\n","for idx, (text_chunks, code_chunks, label) in enumerate(zip(split_text_data, split_code_data, labels)):\n","    code_length = sum(len(chunk) for chunk in code_chunks)\n","    if code_length >= 20000000:\n","        # 데이터 제외\n","        excluded_data_indices.append(idx)\n","        excluded_data.append((text_chunks, code_chunks, label))\n","    else:\n","        # 남은 데이터로 저장\n","        remaining_split_text_data.append(text_chunks)\n","        remaining_split_code_data.append(code_chunks)\n","        remaining_labels.append(label)\n","\n","# 결과 출력\n","print(f\"Number of entries with code length >= 20,000,000: {len(excluded_data_indices)}\")\n","print(\"Indices of excluded entries:\", excluded_data_indices)\n"]},{"cell_type":"markdown","source":["텍스트와 코드 데이터를 Input data 형식에 맞게 결합하는 코드"],"metadata":{"id":"Y877Rh6mLPKY"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"-flSnMosZMWx"},"outputs":[],"source":["def combine_chunks(split_text_data, split_code_data, labels, start_percent=0, end_percent=100):\n","    \"\"\"\n","    분할된 텍스트와 코드 데이터를 합치는 함수.\n","    시작과 끝 인덱스를 기반으로 텍스트와 코드를 결합\n","    \"\"\"\n","    combined_data = []\n","    total_data = len(split_text_data)\n","\n","    # 시작 및 종료 인덱스 계산\n","    start_idx = int(total_data * (start_percent / 100))\n","    end_idx = int(total_data * (end_percent / 100))\n","\n","    for idx, (text_chunks, code_chunks, label) in enumerate(zip(split_text_data[start_idx:end_idx],\n","                                                                split_code_data[start_idx:end_idx],\n","                                                                labels[start_idx:end_idx])):\n","\n","        # 텍스트와 코드 결합\n","        combined_input = \"\"\n","        for text_chunk in text_chunks:\n","            for code_chunk in code_chunks:\n","                combined_input += f\"[CLS] {text_chunk} [SEP] {code_chunk} [EOS] \"\n","\n","        combined_data.append((combined_input.strip(), label))\n","\n","        # 진행 상황 표시: 10% 단위로 업데이트\n","        current_progress = int(((idx + 1) / (end_idx - start_idx)) * 100)\n","        if current_progress % 10 == 0:\n","            print(f\"Combination progress: {current_progress}%\")\n","\n","    print(f\"Combination completed for range {start_percent}% to {end_percent}%.\")\n","    return combined_data\n","\n","combined_remaining_data = combine_chunks(remaining_split_text_data, remaining_split_code_data, remaining_labels)\n","\n","# 결과 출력\n","print(f\"Total combined data for remaining entries: {len(combined_remaining_data)}\")\n"]},{"cell_type":"markdown","source":["아래 코드는 합친 데이터(텍스트 + 코드)를 토큰화함"],"metadata":{"id":"-58ZnEs5K-gJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLiawo_1vHwl"},"outputs":[],"source":["def tokenize_data_for_codebert(combined_data, max_length=512):\n","    \"\"\"\n","    CodeBERT 모델을 위한 데이터를 토큰화하는 함수.\n","    \"\"\"\n","    tokenized_inputs = []\n","\n","    for idx, (combined_input, label) in enumerate(combined_data):\n","        # 텍스트 + 코드 결합 입력을 토큰화\n","        tokenized = tokenizer(\n","            combined_input,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\"\n","        )\n","        # 라벨도 함께 포함하여 저장\n","        tokenized_inputs.append((tokenized, label))\n","\n","        # 진행 상황 표시\n","        if (idx + 1) % (len(combined_data) // 10) == 0:\n","            print(f\"Tokenization progress: {((idx + 1) / len(combined_data)) * 100:.1f}%\")\n","\n","    print(\"Tokenization complete!\")\n","    return tokenized_inputs\n","\n","# combined_remaining_data에서 텍스트와 코드 합친 데이터를 토큰화\n","tokenized_data = tokenize_data_for_codebert(combined_remaining_data)\n"]},{"cell_type":"markdown","source":["encoding한 라벨을 보기 쉽게 한글로 매핑하고 데이터 셋 분할"],"metadata":{"id":"8oZVBraKKxsO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oHeqGCR6dPV"},"outputs":[],"source":["import torch\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","\n","# 라벨을 한국어로 매핑\n","label_mapping = {0: '다크패턴 아닌 데이터', 1: '방해형', 2: '압박형', 3: '오도형'}\n","\n","# Tokenized 데이터를 처리할 Dataset 클래스 정의\n","class CodeBERTDataset(Dataset):\n","    def __init__(self, tokenized_data):\n","        self.tokenized_data = tokenized_data\n","\n","    def __len__(self):\n","        return len(self.tokenized_data)\n","\n","    def __getitem__(self, idx):\n","        tokenized_input, label = self.tokenized_data[idx]\n","        input_ids = tokenized_input['input_ids'].squeeze()\n","        attention_mask = tokenized_input['attention_mask'].squeeze()\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# 토큰화된 데이터로 훈련 데이터와 테스트 데이터로 분할\n","train_tokenized, test_tokenized, train_labels, test_labels = train_test_split(\n","    tokenized_data, remaining_labels, test_size=0.2, stratify=remaining_labels\n",")\n","\n","# Dataset 생성\n","train_dataset = CodeBERTDataset(train_tokenized)\n","test_dataset = CodeBERTDataset(test_tokenized)"]},{"cell_type":"markdown","source":["아래 코드는 CodeBERT 모델 학습시키고 성능 평가"],"metadata":{"id":"Z9R7ak3tKmit"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hIZMtWP6iNs"},"outputs":[],"source":["# 모델과 훈련 파라미터 설정\n","model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=4)\n","\n","training_args = TrainingArguments(\n","    num_train_epochs=5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    learning_rate=2e-5,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    evaluation_strategy=\"epoch\",\n","    report_to='none'\n",")\n","\n","# Trainer 객체 생성\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")\n","\n","# 모델 학습\n","trainer.train()\n","\n","# 평가 (테스트 데이터에 대한 예측 수행)\n","predictions, true_labels, metrics = trainer.predict(test_dataset)\n","\n","# 예측 결과를 라벨로 변환 (0, 1, 2, 3 -> 한국어 라벨)\n","predicted_labels = [label_mapping[label] for label in predictions.argmax(axis=1)]\n","true_labels = [label_mapping[label] for label in true_labels]\n","\n","# 전체 정확도 계산\n","correct_predictions = [pred == true for pred, true in zip(predicted_labels, true_labels)]\n","overall_accuracy = np.mean(correct_predictions)\n","print(f\"전체 정확도: {overall_accuracy * 100:.2f}%\")\n","\n","# 각 클래스별 정확도 계산\n","class_correct_counts = {label: 0 for label in label_mapping.values()}\n","class_total_counts = {label: 0 for label in label_mapping.values()}\n","\n","# 클래스별로 맞게 예측된 수 계산\n","for pred, true in zip(predicted_labels, true_labels):\n","    class_total_counts[true] += 1\n","    if pred == true:\n","        class_correct_counts[true] += 1\n","\n","# 각 클래스별 정확도 출력\n","print(\"\\n각 클래스별 정확도:\")\n","for label in label_mapping.values():\n","    accuracy = (class_correct_counts[label] / class_total_counts[label]) * 100\n","    print(f\"{label}: {accuracy:.2f}% ({class_correct_counts[label]}/{class_total_counts[label]}개 맞음)\")\n","\n","# classification_report 출력 (한국어 라벨로 출력)\n","print(\"\\nClassification Report:\")\n","print(classification_report(true_labels, predicted_labels, target_names=list(label_mapping.values())))"]},{"cell_type":"markdown","source":["아래 코드는 테스트 데이터에서 각 유형별 수 계산\n","학습시키는 것과 관련 없음, 확인용 코드"],"metadata":{"id":"afb65yo0KSbT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtJkALIg6PXV"},"outputs":[],"source":["from collections import Counter\n","\n","# 테스트 데이터 라벨에 대한 빈도 계산\n","test_label_counts = Counter(test_labels)\n","\n","mapped_test_label_counts = {label_mapping[label]: count for label, count in test_label_counts.items()}\n","\n","print(\"테스트 데이터에서 각 유형별 데이터 수:\")\n","for label, count in mapped_test_label_counts.items():\n","    print(f\"{label}: {count}개\")\n"]}]}